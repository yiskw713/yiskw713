Hi thereğŸ‘‹, I'm Yuchi Ishikawa (çŸ³å· è£•åœ°).

* ğŸ‘¨â€ğŸ’» I'm currently working as a research engineer at LY Corporation while pursuing my PhD in Japan, majoring in Computer Vision and Machine Learning.
* ğŸ§ª My research topic is about video understanding and self-supervised learning.
* ğŸ’ I love traveling all over the world. I've been to over 30 countries and landed on the seven continents so far.
* ğŸºğŸ¶ I'm really into Japanese sake, and I also work at a sake shop as a side job, helping customers with sales and recommendations.

### Languages and Skills

<p>
<img src="https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
<img src="https://img.shields.io/badge/-PyTorch-EE4C2C?style=flat-square&logo=PyTorch&logoColor=white"/>
<img src="https://img.shields.io/badge/-pandas-150458?style=flat-square&logo=pandas&logoColor=white"/>
<img src="https://img.shields.io/badge/-Django-092E20?style=flat-square&logo=Django&logoColor=white"/>
<img src="https://img.shields.io/badge/-Rust-000000?style=flat-square&logo=Rust&logoColor=white"/>
<img src="https://img.shields.io/badge/-JavaScript-F7DF1E?style=flat-square&logo=JavaScript&logoColor=black"/>
<img src="https://img.shields.io/badge/-TypeScript-007ACC?style=flat-square&logo=TypeScript&logoColor=white"/>
<img src="https://img.shields.io/badge/-Vue.js-42B883?style=flat-square&logo=Vue-dot-js&logoColor=white"/>
<img src="https://img.shields.io/badge/-Nuxt.js-00C58E?style=flat-square&logo=nuxt-dot-js&logoColor=white"/>
<img src="https://img.shields.io/badge/-C++-00599C?style=flat-square&logo=c%2B%2B&logoColor=white"/>
<img src="https://img.shields.io/badge/-HTML5-E34F26?style=flat-square&logo=HTML5&logoColor=white"/>
<img src="https://img.shields.io/badge/-CSS3-1572B6?style=flat-square&logo=CSS3&logoColor=white"/>
<img src="https://img.shields.io/badge/-Sass-1572B6?style=flat-square&logo=SASS&logoColor=white"/>
<img src="https://img.shields.io/badge/-MySQL-F29111?style=flat-square&logo=MySQL&logoColor=white"/>
<img src="https://img.shields.io/badge/-PostgreSQL-F29111?style=flat-square&logo=PostgreSQL&logoColor=white"/>
<img src="https://img.shields.io/badge/-Visual%20Studio%20Code-23A9F2?style=flat-square&logo=Visual%20Studio%20Code&logoColor=white"/>
<img src="https://img.shields.io/badge/-Vim-1572B6?style=flat-square&logo=Vim&logoColor=white"/>
<img src="https://img.shields.io/badge/-Github-181717?style=flat-square&logo=GitHub&logoColor=white"/>
<img src="https://img.shields.io/badge/-Git-F44D27?style=flat-square&logo=Git&logoColor=white"/>
<img src="https://img.shields.io/badge/-Google%20Cloud-4285F4?style=flat-square&logo=Google%20Cloud&logoColor=white"/>
<img src="https://img.shields.io/badge/-Amazon%20AWS-232F3E?style=flat-square&logo=Amazon%20AWS&logoColor=white"/>
<img src="https://img.shields.io/badge/-Docker-2496ED?style=flat-square&logo=Docker&logoColor=white"/>
</p>

### Where to Find Me ğŸ‘€

[<img align="left" width="40px" src="https://www.svgrepo.com/show/349396/google-scholar.svg" />][googlescholar]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/twitter-x.svg" />][twitter]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/linkedin.svg" />][linkedin]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/instagram.svg" />][instagram]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/facebook.svg" />][facebook]
[<img align="left" width="40px" src="https://cdn.icon-icons.com/icons2/1996/PNG/512/blog_blogger_business_news_web_website_icon_123264.png" />][blog]

<br>
<br>

âœ‰ï¸ : yishikawa[at]aoki-medialab.jp


### Profile ğŸ“–

<!-- Education -->
<details>
<summary>Education</summary>

* **Ph.D. program in Center for Electronics and Electrical Engineering,**
  **School of Integrated Design Engineering,**
  **Graduate School of Keio University** (Apr. 2021 â€“ present in Japan) <br>
  *My research theme is about human motion analysis and removing scene bias in action recognition. (Adviser: Prof. Yoshimitsu AOKI. [Lab Homepage](https://aoki-medialab.jp/))*

* **M.S. in Center for Electronics and Electrical Engineering,**
  **School of Integrated Design Engineering,**
  **Graduate School of Keio University** (Apr. 2019 â€“ Mar. 2021 in Japan) <br>
  *My research theme is about human motion analysis and object function detection. Iâ€™m expected to get a masterâ€™s degree in 2021. (Adviser: Prof. Yoshimitsu AOKI. [Lab Homepage](https://aoki-medialab.jp/))*

* **B.S. in Dept. of Electronics and Electrical Engineering,**
  **Keio University** (Apr. 2015 - Mar.2019 in Japan) <br>
  *I mainly studied Electronics in the first three years. During the last year, I researched Machine Learning, Computer Vision and Robot Vision. (Adviser: Prof. Yoshimitsu AOKI. [Lab Homepage](https://aoki-medialab.jp/))*

</details>

<!-- Experience -->
<details>
<summary>Experience</summary>

* **Machine Learning Research Engineer in [LY Corporation](https://www.lycorp.co.jp/en/) [Oct. 2023 - present in Tokyo, Japan]**<br>
  Computer Vision

* **Sake Sales Staff (Part-time) [Feb. 2025 - present in Tokyo, Japan]**<br>
Japanese sake sales and customer service

* **Machine Learning Research Engineer in [LINE Corp.](https://linecorp.com/en/) [Oct. 2022 - Sep. 2023 in Tokyo, Japan]**<br>
  Computer Vision

* **Technical Advisor in [BizTech, Inc.](https://www.biz-t.jp/) [Apr. 2022 - present in Japan]**<br>

* **Machine Learning Engineer in [Softbank Corp.](https://www.softbank.jp/en/) [Apr. 2021 - Sep. 2022 in Tokyo, Japan]**<br>
  Computer Vision / Edge Device Development / Backend Engineer

* **Backend Engineer developing [AIC website](https://aic.keio.ac.jp/forStudents/web) [Sep. 2020 - present in Japan]**<br>

* **Internship in [Tenchijin Inc.](https://tenchijin.co.jp/) [Jun. 2020 â€“ Mar. 2021  in Tokyo, Japan]**<br>
  Backend / Machine Learning Engineer working on the analysis of the big data about space.

* **Internship in [CyberAgent, Inc.](https://www.cyberagent.co.jp/en/) [Feb. 2020 - Feb. 2020  Tokyo, Japan]**<br>
  Worked on developing AdTech using machine learning and GCP.

* **Internship in [SoftBank Corp.](https://www.softbank.jp/en/) [Aug. 2019 â€“ Sep. 2019 in Tokyo, Japan]**<br>
  Worked on the following two task:<br>
  1. classifying a product into normal one or abnormal one and visualizing where a CNN model looks<br>
  2. semantic segmentation for super high-resolution images<br>

* **Research Assisstant in [National Institute of Advanced Industrial Science and Technology(AIST)](https://www.aist.go.jp/index_en.html) [Apr. 2019 â€“ Mar. 2021 in Tsukuba, Ibaraki, Japan]**<br>
  Research about Machine Learning and Action Recognition under the supervision of [Ph.D. Hirokatsu KATAOKA](http://hirokatsukataoka.net/).

* **Internship in [IBM Japan, Ltd.](https://www.ibm.com/ibm/jp/en/) [Sep. 2018 - Mar. 2019 in Tokyo, Japan]**<br>
  Worked on weakly-supervised affordance detection using the hierarchy between affordances and objects.

</details>

<!-- Publications -->
<details>
<summary>Publications</summary>

#### Preprint

* **Yuchi Ishikawa**, Toranosuke Manabe, Tatsuya Komatsu, Yoshimitsu Aoki, "Listening without Looking: Modality Bias in Audio-Visual Captioning", in arXiv 2025.

* Toranosuke Manabe, **Yuchi Ishikawa**, Hokuto Munakata, Tatsuya Komatsu, "ProLAP: Probabilistic Language-Audio Pre-Training", in arXiv 2025. [paper](https://arxiv.org/abs/2510.18423)

#### International Conference

* Tatsuya Komatsu, Hokuto Munakata, **Yuchi Ishikawa**, "Leveraging Unlabeled Audio for Audio-Text Contrastive Learning via Audio-Composed Text Features", in Interspeech 2025.

* **Yuchi Ishikawa**, Shota Nakada, Hokuto Munakata, Kazuhiro Saito, Tatsuya Komatsu, Yoshimitsu Aoki, "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", in Interspeech 2025. [paper](https://arxiv.org/abs/2507.11967)

* **Yuchi Ishikawa**, Tatsuya Komatsu, Yoshimitsu Aoki, "Pre-training with Synthetic Patterns for Audio", in ICASSP 2025. [paper](https://www.arxiv.org/abs/2410.00511)

* **Yuchi Ishikawa**, Masayoshi Kondo, Yoshimitsu Aoki, "Data Collection-free Masked Video Modeling" in ECCV 2024. [paper](https://arxiv.org/abs/2409.06665)

* **Yuchi Ishikawa**, Masayoshi Kondo, Hirokatsu Kataoka, "Learnable Cube-based Video Encryption for Privacy-Preserving Action Recognition" in WACV 2024. [paper](https://openaccess.thecvf.com/content/WACV2024/papers/Ishikawa_Learnable_Cube-Based_Video_Encryption_for_Privacy-Preserving_Action_Recognition_WACV_2024_paper.pdf)

* Shuhei Yokoo, Peifei Zhu, **Yuchi Ishikawa**, Mikihiro Tanaka, Masayoshi Kondo, Hirokatsu Kataoka, "Leveraging Image-Text Similarity and Caption Modification for the DataComp Challenge: Filtering Track and BYOD Track" in ICCV 2023 Workshop on Towards the Next Generation of Computer Vision Datasets: DataComp Track. [arXiv](https://arxiv.org/abs/2310.14581)

* Kensho Hara, **Yuchi Ishikawa**, Hirokatsu Kataoka, "Rethinking Training Data for Mitigating Representation Biases in Action Recognition" in CVPR 2021 Workshop on Large Scale Holistic Video Understanding 2021

* **Yuchi Ishikawa**, Seito Kasai, Yoshimitsu Aoki, Hirokatsu kataoka, "Alleviating Over-segmentation Errors by Detecting Action Boundaries" in WACV 2021. [arXiv](https://arxiv.org/abs/2007.06866)

* Seito Kasai, **Yuchi Ishikawa**, Masaki Hayashi, Yoshimitsu Aoki, Kensho Hara, Hirokatsu Kataoka, â€œRETRIEVING AND HIGHLIGHTING ACTION WITH SPATIOTEMPORAL REFERENCEâ€ in IEEE ICIP 2020. [arXiv](https://arxiv.org/abs/2005.09183?context=cs)

* **Yuchi Ishikawa**, Haruya Ishikawa, Shuichi Akizuki, Masaki Yamazaki, Yasuhiro Taniguchi, Yoshimitsu Aoki, "Task-oriented Function Detection Based on Operational Tasks" in Conference: 2019 19th International Conference on Advanced Robotics (ICAR). (Acceptance Rate 55.3%)

* Seito Kasai\*, **Yuchi Ishikawa\***, Tenga Wakamiya, Kensho Hara, Hirokatsu Kataoka, â€œAIST Team submission for Task 3, Dense-Captioning Events in Videos,â€  in CVPR 2019 Workshop, International Challenge on ActivityNet Challenge, 2019.

* Tenga Wakamiya, Kensho Hara, **Yuchi Ishikawa**, Seito Kasai, Hirokatsu Kataoka, â€œAIST Submission for ActivityNet Challenge 2019 in Trimmed Activity Recognition (Kinetics),â€ in CVPR 2019 Workshop, International Challenge on ActivityNet Challenge, 2019.

* Haruya Ishikawa, **Yuchi Ishikawa**, Shuichi Akizuki, Yoshimitsu Aoki, "Human-Object Maps for Daily Activity Recognition" in International Conference on Machine Vision Applications(MVA 2019)

#### Journal

* **çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œç§‹æœˆç§€ä¸€ï¼Œé’æœ¨ç¾©æº€ï¼Œæ“ä½œã‚¿ã‚¹ã‚¯å…¥åŠ›ã«åŸºã¥ãç‰©ä½“ã®æ©Ÿèƒ½éƒ¨æ¨å®š, ç²¾å¯†å·¥å­¦ä¼š 85å·»12å· (2019å¹´12æœˆ5æ—¥ç™ºè¡Œ)

#### Domestic Conference

* **çŸ³å· è£•åœ°**, å°æ¾ é”ä¹Ÿ, ä»²ç”° å‹å¤ª, å®—åƒ åŒ—æ–—, é½‹è—¤ ä¸»è£•, é’æœ¨ ç¾©æº€, "è¦–è´è¦šèªè­˜ã®ãŸã‚ã®ãƒˆãƒªãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã¨è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®ææ¡ˆ", ç¬¬28å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2025)

* é½‹è—¤ ä¸»è£•, å°æ¾ é”ä¹Ÿ, **çŸ³å· è£•åœ°**, è¿‘è—¤ é›…èŠ³,"ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®ãŸã‚ã®Vision Transformerãƒ¢ãƒ‡ãƒ«ã®æš—å·åŒ–æ³•", ç¬¬28å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2025)

* **çŸ³å· è£•åœ°**, é½‹è—¤ä¸»è£•, é’æœ¨ç¾©æº€, "å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã¨ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’ç”¨ã„ãŸéŸ³ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®è‡ªå‹•ç”Ÿæˆ", è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š(NLP2025)

* **â½¯å· è£•åœ°**ï¼Œâ»˜â½Š ç¾©æº€, æš—å·åŒ–å‹•ç”»ã‚’â½¤ã„ãŸãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ä¸‹ã§ã®è¦–è´è¦šâ¾å‹•èªè­˜, ãƒ“ã‚¸ãƒ§ãƒ³æŠ€è¡“ã®å®Ÿåˆ©ç”¨ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2024 (ViEW 2024)

* é½‹è—¤ ä¸»è£•ï¼Œ**â½¯å· è£•åœ°**, Pseudo-Motion Videoã®å¤šæ§˜åŒ–ã«ã‚ˆã‚‹å‹•ç”»èªè­˜ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã®â¾¼ç²¾åº¦åŒ–ã®æ¤œè¨, ãƒ“ã‚¸ãƒ§ãƒ³æŠ€è¡“ã®å®Ÿåˆ©ç”¨ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2024 (ViEW 2024)

* Naoya Nakajima, **Yuchi Ishikawa**, Masayoshi Kondo, å‹•ç”»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ãŸã‚ã®DINOã®å‹•ç”»ã¸ã®æ‹¡å¼µã®æ¤œè¨, ç¬¬27å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2024)

* **çŸ³å· è£•åœ°**, è¿‘è—¤ é›…èŠ³, é’æœ¨ ç¾©æº€, ä¸‰é‡ãƒã‚¹ã‚¯ã‚’ç”¨ã„ãŸVideoMAEã®åŠ¹ç‡åŒ–, å‹•çš„ç”»åƒå‡¦ç†å®Ÿåˆ©ç”¨åŒ–ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2024 (DIA2024)

* **çŸ³å· è£•åœ°**, è¿‘è—¤ é›…èŠ³, é’æœ¨ ç¾©æº€, è‡ªç„¶å‹•ç”»ã‚’ç”¨ã„ãªã„å‹•ç”»èªè­˜ãƒ¢ãƒ‡ãƒ«ã®è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’, å‹•çš„ç”»åƒå‡¦ç†å®Ÿåˆ©ç”¨åŒ–ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2024 (DIA2024)

* **çŸ³å· è£•åœ°**, è¿‘è—¤ é›…èŠ³, é’æœ¨ ç¾©æº€, Training Video Masked Autoencoder from Static Images, in ãƒ“ã‚¸ãƒ§ãƒ³æŠ€è¡“ã®å®Ÿåˆ©ç”¨ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2023 (ViEW 2023)

* **Yuchi Ishikawa**, Masayoshi Kondo, Hirokatsu Kataoka, Video and Model Encryption for Privacy-Preserving Action Recognition, ç¬¬26å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2023)

* **çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œç§‹æœˆç§€ä¸€ï¼Œé’æœ¨ç¾©æº€ï¼ŒAction Segmentation ã«ãŠã‘ã‚‹æå¤±é–¢æ•°ã®æ¤œè¨¼ï¼Œç¬¬26å›ç”»åƒã‚»ãƒ³ã‚·ãƒ³ã‚°ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ (SSII 2020)

* Seito KASAI, **Yuchi ISHIKAWA**, Tenga WAKAMIYA, Kensho HARA, Hirokatsu KATAOKA, Exploring the Best Model for Dense Captioning Events in Videos, ç¬¬22å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2019)

* è‹¥å®®å¤©é›…ï¼ŒåŸå¥ç¿”ï¼Œ**çŸ³å·è£•åœ°**ï¼Œç¬ äº•èª æ–—ï¼Œä¸­æ‘ æ˜ç”Ÿï¼Œç‰‡å²¡ è£•é›„ï¼Œè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹å‹•ç”»åƒèªè­˜ã®ãŸã‚ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼Œç¬¬22å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2019)

* **çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œç§‹æœˆç§€ä¸€ï¼Œé’æœ¨ç¾©æº€ï¼Œãƒ­ãƒ›ã‚™ãƒƒãƒˆã®ç‰©ä½“æ“ä½œã®ãŸã‚ã®ã‚¿ã‚¹ã‚¯æŒ‡å‘ãªæ©Ÿèƒ½éƒ¨ã®æ¨å®šï¼Œç¬¬22å›ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ  (MIRU2019)

* ç§‹æœˆç§€ä¸€ï¼Œ**çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œé’æœ¨ç¾©æº€ï¼Œç‰©ä½“ã®é…ç½®è¨˜è¿°ã«åŸºã¤ã‚™ãã‚·ãƒ¼ãƒ³å¾©å…ƒã®ãŸã‚ã®æ“ä½œæ–¹æ³•æ¨å®šï¼Œç¬¬25å›ç”»åƒã‚»ãƒ³ã‚·ãƒ³ã‚°ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ (SSII 2019)

* **çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œç§‹æœˆç§€ä¸€ï¼Œé’æœ¨ç¾©æº€ï¼Œæ“ä½œã‚¿ã‚¹ã‚¯ã¨æŒ‡ç¤ºå¯¾è±¡ã‚¯ãƒ©ã‚¹ã®å…¥åŠ›ã«ã‚ˆã‚‹ç‰©ä½“ã®æ©Ÿèƒ½éƒ¨æ¨å®šï¼Œç¬¬25å›ç”»åƒã‚»ãƒ³ã‚·ãƒ³ã‚°ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ (SSII 2019)

* **çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œç§‹æœˆç§€ä¸€ï¼Œé’æœ¨ç¾©æº€ï¼Œæ“ä½œã‚¿ã‚¹ã‚¯å…¥åŠ›ã«åŸºã¥ãç‰©ä½“ã®æ©Ÿèƒ½éƒ¨æ¨å®šï¼Œå‹•çš„ç”»åƒå‡¦ç†å®Ÿåˆ©ç”¨åŒ–ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2019(DIA 2019)

* ç§‹æœˆç§€ä¸€ï¼Œ**çŸ³å·è£•åœ°**ï¼ŒçŸ³å·æ™´ä¹Ÿï¼Œé’æœ¨ç¾©æº€ï¼Œéå®šå¸¸çŠ¶æ…‹ã®ç†è§£ã¨ã‚·ãƒ¼ãƒ³å¾©å…ƒã®ãŸã‚ã®ç‰©ä½“ã®æ“ä½œæ–¹æ³•æ¨å®šï¼Œå‹•çš„ç”»åƒå‡¦ç†å®Ÿåˆ©ç”¨åŒ–ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—2019(DIA 2019)
</details>

<!-- Projects -->
<details>
<summary>Projects</summary>

* **[cvpaper.challenge ç ”ç©¶åŠ¹ç‡åŒ– Tips](https://www.slideshare.net/cvpaperchallenge/cvpaperchallenge-tips-241914101)**<br>
  I wrote about how to efficiently manage experiment results.
  
* **[å‹•ç”»èªè­˜ ãƒ¡ã‚¿ã‚µãƒ¼ãƒ™ã‚¤](https://www.slideshare.net/cvpaperchallenge/v1-232973484)** [May. 2020]<br>
  I read papers on Video Recognition and summarized them.

* **[RADTorch](https://github.com/radtorch/radtorch) Contributor**<br>
  My codes for explainable AI are used in RADTorch.

* **[CVPR2019é€Ÿå ±](https://www.slideshare.net/cvpaperchallenge/cvpr-2019)**<br>
  I took part in CVPR 2019 and wrote this article with members of cvpaper.challenge.

* **[cvpaper.challenge](http://xpaperchallenge.org/cv/) [Apr. 2019 â€“ present]**<br>
  As a member of cvpaper.challenge, I read a lot of papers accepted at CVPR or several top conferences. I also reseach and share the knowledge with its members.

* **[ActivityNet Challenge](http://activity-net.org/) [Jun. 2019]**<br>
  Out team took part in ActivityNet Challenge in CVPR workshop. We won 9th place in Task A - Trimmed Action Recognition. Our team also participated in Task 3 - Dense-Captioning Events in Videos.

* **[Paper Summary](https://github.com/yiskw713/paper_summary)**<br>
  I read papers every day and summarize them as far as possible in GitHub. If you get interested, visit [my project page](https://github.com/yiskw713/paper_summary).

</details>

<!-- Article -->
<details>
<summary>Articles</summary>

* [Seven papers accepted at ICASSP 2025](https://research.lycorp.co.jp/en/news/283)

* [3 papers accepted at ECCV 2024](https://research.lycorp.co.jp/en/news/250)

* **Interview with a manager of the AI department in Softbank**<br>
  [SFæ˜ ç”»ã®ä¸–ç•Œã¯ã‚‚ã†å®Ÿç¾ã—ã¦ã„ã‚‹ï¼Ÿ ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ AIéƒ¨é–€ éƒ¨é•·ã¨AIå°‚æ”»å­¦ç”Ÿã®å¯¾è«‡](https://www.softbank.jp/sbnews/entry/20191108_01)

</details>

<!-- Recognition -->
<details>
<summary>Recognition</summary>

* **[æ—¥æœ¬é…’æ¤œå®š 2ç´š](https://ssi-w.com/nihonsyu-kentei/) [Feb. 2025]**

* **[æ—¥æœ¬é…’æ¤œå®š 3ç´š](https://ssi-w.com/nihonsyu-kentei/) [Jan. 2025]**

* **[ã‚³ãƒ¼ãƒ’ãƒ¼ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå®š 2ç´š](https://kentei.jcqa.org/index.asp) [Jun. 2024]**
  
* **[PythonZen & PEP 8 æ¤œå®šè©¦é¨“](https://pythonzen-pep8-exam.jp/) [Mar. 2022]**
  
* **[JDLA Deep Learning for ENGINEER](https://www.jdla.org/en/) [Feb. 2022]**

* **[Python 3 ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢èªå®šãƒ‡ãƒ¼ã‚¿åˆ†æè©¦é¨“](https://www.pythonic-exam.com/exam/analyist) [Aug. 2021]**

* **[AWS Cloud Practitioner](https://aws.amazon.com/jp/certification/certified-cloud-practitioner/) [Jul. 2021]**

* **[Python 3 ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢èªå®šåŸºç¤è©¦é¨“](https://www.pythonic-exam.com/exam/basic) [Jul. 2021]**

* **[Practical Algorithm Skill Test](https://past.atcoder.jp/) [May. 2020]**<br>
  Rank: ADVANCED (the second highest rank)

* **[Information Technology Passport Examination (IT Passport)](https://www3.jitec.ipa.go.jp/JitesCbt/index.html) [Jan. 2020]**<br>
  National Examination on Information Technology in Japan

* **[JDLA Deep Learning for GENERAL](https://www.jdla.org/en/) [Jul. 2019]**<br>
  JDLA aims to develop Deep Learning Generalist, capable of utilizing in business, which has sufficient knowledge in Deep Leaning.

* **Outstanding Performance Award for Bachelor Theses [Mar. 2019]**<br>
  My bachelor thesis topic is â€œTask-oriented Function Detection Based on Operational Tasksâ€.
  I proposed an alternative representation to Affordance, â€œTask-oriented Functionâ€, in the paper. This representation makes it possible to desribe a variety of ways to use an object, though only one usage can be described in Affordance Detection in the field of Computer Vision.

* **3rd place award in MIRU 2018 Young Researchers Program [Aug. 2018]**<br>
  As a young researcher program in MIRU 2018, participants were divided into several groups and each group read papers in the field outside Computer Vision. Then, each group summarized the history, the trend and the conection with Computer Vision. It helped us not only understand different fields, but also consider how we can make use of knowledge about them for Computer Vision.<br>
  Our group, C, read papers in the field of Robotics, focusing on â€œTransferring Knowledge from Simulation to Realâ€. You can see our poster and presentation material from [this link](https://sites.google.com/view/miru2018sapporo/wakate_top/%E5%90%84%E3%83%81%E3%83%BC%E3%83%A0%E3%81%AE%E7%99%BA%E8%A1%A8%E8%B3%87%E6%96%99?authuser=0).

* **[TOEIC](https://www.iibc-global.org/english.html) [May 2018]**<br>
  Score: 940

</details>

<!-- Grants -->
<details>
<summary>Grants</summary>

* **[æ…¶å¿œå·¥å­¦ä¼šè‚²è‹±å¥¨å­¦ç”Ÿ](http://www.keiokougakukai.org/) [Apr. 2021 - Mar. 2022]**

* **[Scholarship of Japan Student Services Organization](https://www.jasso.go.jp/shogakukin/index.html) [Apr. 2019 - Mar. 2021]**<br>
  This educational lender have been totally exempted because my achievement during master program was highly evaluated.

* **JEESãƒ»ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯AIäººæè‚²æˆå¥¨å­¦é‡‘ [Apr. 2019 - Mar. 2020]**<br>
  I received this scholorship which aims at supporting up to a hundred of capable students studying Artificial Intelligence in Japan.

</details>

### GitHub Streaks ğŸƒâ€â™‚ï¸

[![GitHub Streak](http://github-readme-streak-stats.herokuapp.com?user=yiskw713&theme=gotham&hide_border=true)](https://git.io/streak-stats)

### GitHub Stats âœ¨

<img align="left" src="https://github-readme-stats.vercel.app/api?username=yiskw713&count_private=true&show_icons=true&theme=gotham&include_all_commits=true" />
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=yiskw713&hide=jupyter%20notebook&layout=compact&theme=gotham" />

<br />

### GitHub Trophy ğŸ†

[![trophy](https://github-profile-trophy.vercel.app/?username=yiskw713&theme=alduin&column=8)](https://github.com/ryo-ma/github-profile-trophy)

### Latest Blog Posts ğŸ“•

<!-- BLOG-POST-LIST:START -->
- [ã€Pythonã€‘ç‰¹å®šã®è¡Œã ã‘Blackã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰æ•´å½¢ã‚’è¡Œã‚ãªã„ã‚ˆã†ã«ã™ã‚‹](https://yiskw713.hatenablog.com/entry/2023/05/08/190000)
- [ã€Pythonã€‘å®šæ•°ç®¡ç†ã®ãŸã‚ã®ã€å€¤ã®è¿½åŠ ãƒ»å¤‰æ›´ã‚’ä¸å¯ã«ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã™ã‚‹](https://yiskw713.hatenablog.com/entry/2023/05/05/182048)
- [ã€numpyã€‘ç‰¹å®šã®ç¯„å›²ã ã‘ã«seedã‚’è¨­å®šã™ã‚‹](https://yiskw713.hatenablog.com/entry/2023/02/06/190000)
- [ã€Linuxã€‘ãƒã‚¤ãƒ•ãƒ³ã‹ã‚‰å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹](https://yiskw713.hatenablog.com/entry/2023/02/03/120000)
- [ã€Pythonã€‘boxã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹](https://yiskw713.hatenablog.com/entry/2023/01/30/200000)
<!-- BLOG-POST-LIST:END -->

### GitHub Activities

<!--START_SECTION:activity-->
1. ğŸ—£ Commented on [#1](https://github.com/yiskw713/paper-summary-tweet-notifier/issues/1) in [yiskw713/paper-summary-tweet-notifier](https://github.com/yiskw713/paper-summary-tweet-notifier)
2. ğŸ—£ Commented on [#2](https://github.com/yiskw713/paper-summary-tweet-notifier/issues/2) in [yiskw713/paper-summary-tweet-notifier](https://github.com/yiskw713/paper-summary-tweet-notifier)
3. ğŸ’ª Opened PR [#21](https://github.com/yiskw713/pytorch_template/pull/21) in [yiskw713/pytorch_template](https://github.com/yiskw713/pytorch_template)
4. ğŸ’ª Opened PR [#8](https://github.com/yiskw713/cv_utils/pull/8) in [yiskw713/cv_utils](https://github.com/yiskw713/cv_utils)
5. â—ï¸ Opened issue [#171](https://github.com/yiskw713/paper_summary/issues/171) in [yiskw713/paper_summary](https://github.com/yiskw713/paper_summary)
<!--END_SECTION:activity-->

[googlescholar]: https://scholar.google.com/citations?user=IEF2iOkAAAAJ&hl=en&oi=ao
[twitter]: https://twitter.com/yiskw713
[instagram]: https://www.instagram.com/yciskw_/
[linkedin]: https://www.linkedin.com/in/yiskw713/
[facebook]: https://www.facebook.com/yuchi.ishikawa.7
[blog]: https://yiskw713.hatenablog.com/
