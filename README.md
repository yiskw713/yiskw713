Hi thereüëã, I'm Yuchi Ishikawa (Áü≥Â∑ù Ë£ïÂú∞).

* üë®‚Äçüíª I'm currently working as a research engineer at LY Corporation while pursuing my PhD in Japan, majoring in Computer Vision and Machine Learning.
* üß™ My research topic is about video understanding and self-supervised learning.
* üéí I love traveling all over the world. I've been to over 30 countries and landed on the seven continents so far.
* üç∫üç∂ I'm really into Japanese sake, and I also work at a sake shop as a side job, helping customers with sales and recommendations.

### Languages and Skills

<p>
<img src="https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
<img src="https://img.shields.io/badge/-PyTorch-EE4C2C?style=flat-square&logo=PyTorch&logoColor=white"/>
<img src="https://img.shields.io/badge/-pandas-150458?style=flat-square&logo=pandas&logoColor=white"/>
<img src="https://img.shields.io/badge/-Django-092E20?style=flat-square&logo=Django&logoColor=white"/>
<img src="https://img.shields.io/badge/-Rust-000000?style=flat-square&logo=Rust&logoColor=white"/>
<img src="https://img.shields.io/badge/-JavaScript-F7DF1E?style=flat-square&logo=JavaScript&logoColor=black"/>
<img src="https://img.shields.io/badge/-TypeScript-007ACC?style=flat-square&logo=TypeScript&logoColor=white"/>
<img src="https://img.shields.io/badge/-Vue.js-42B883?style=flat-square&logo=Vue-dot-js&logoColor=white"/>
<img src="https://img.shields.io/badge/-Nuxt.js-00C58E?style=flat-square&logo=nuxt-dot-js&logoColor=white"/>
<img src="https://img.shields.io/badge/-C++-00599C?style=flat-square&logo=c%2B%2B&logoColor=white"/>
<img src="https://img.shields.io/badge/-HTML5-E34F26?style=flat-square&logo=HTML5&logoColor=white"/>
<img src="https://img.shields.io/badge/-CSS3-1572B6?style=flat-square&logo=CSS3&logoColor=white"/>
<img src="https://img.shields.io/badge/-Sass-1572B6?style=flat-square&logo=SASS&logoColor=white"/>
<img src="https://img.shields.io/badge/-MySQL-F29111?style=flat-square&logo=MySQL&logoColor=white"/>
<img src="https://img.shields.io/badge/-PostgreSQL-F29111?style=flat-square&logo=PostgreSQL&logoColor=white"/>
<img src="https://img.shields.io/badge/-Visual%20Studio%20Code-23A9F2?style=flat-square&logo=Visual%20Studio%20Code&logoColor=white"/>
<img src="https://img.shields.io/badge/-Vim-1572B6?style=flat-square&logo=Vim&logoColor=white"/>
<img src="https://img.shields.io/badge/-Github-181717?style=flat-square&logo=GitHub&logoColor=white"/>
<img src="https://img.shields.io/badge/-Git-F44D27?style=flat-square&logo=Git&logoColor=white"/>
<img src="https://img.shields.io/badge/-Google%20Cloud-4285F4?style=flat-square&logo=Google%20Cloud&logoColor=white"/>
<img src="https://img.shields.io/badge/-Amazon%20AWS-232F3E?style=flat-square&logo=Amazon%20AWS&logoColor=white"/>
<img src="https://img.shields.io/badge/-Docker-2496ED?style=flat-square&logo=Docker&logoColor=white"/>
</p>

### Where to Find Me üëÄ

[<img align="left" width="40px" src="https://www.svgrepo.com/show/349396/google-scholar.svg" />][googlescholar]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/twitter-x.svg" />][twitter]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/linkedin.svg" />][linkedin]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/instagram.svg" />][instagram]
[<img align="left" width="40px" src="https://raw.githubusercontent.com/CLorant/readme-social-icons/main/medium/colored/facebook.svg" />][facebook]
[<img align="left" width="40px" src="https://cdn.icon-icons.com/icons2/1996/PNG/512/blog_blogger_business_news_web_website_icon_123264.png" />][blog]

<br>
<br>

‚úâÔ∏è : yishikawa[at]aoki-medialab.jp


### Profile üìñ

<!-- Education -->
<details>
<summary>Education</summary>

* **Ph.D. program in Center for Electronics and Electrical Engineering,**
  **School of Integrated Design Engineering,**
  **Graduate School of Keio University** (Apr. 2021 ‚Äì present in Japan) <br>
  *My research theme is about human motion analysis and removing scene bias in action recognition. (Adviser: Prof. Yoshimitsu AOKI. [Lab Homepage](https://aoki-medialab.jp/))*

* **M.S. in Center for Electronics and Electrical Engineering,**
  **School of Integrated Design Engineering,**
  **Graduate School of Keio University** (Apr. 2019 ‚Äì Mar. 2021 in Japan) <br>
  *My research theme is about human motion analysis and object function detection. I‚Äôm expected to get a master‚Äôs degree in 2021. (Adviser: Prof. Yoshimitsu AOKI. [Lab Homepage](https://aoki-medialab.jp/))*

* **B.S. in Dept. of Electronics and Electrical Engineering,**
  **Keio University** (Apr. 2015 - Mar.2019 in Japan) <br>
  *I mainly studied Electronics in the first three years. During the last year, I researched Machine Learning, Computer Vision and Robot Vision. (Adviser: Prof. Yoshimitsu AOKI. [Lab Homepage](https://aoki-medialab.jp/))*

</details>

<!-- Experience -->
<details>
<summary>Experience</summary>

* **Machine Learning Research Engineer in [LY Corporation](https://www.lycorp.co.jp/en/) [Oct. 2023 - present in Tokyo, Japan]**<br>
  Computer Vision

* **Sake Sales Staff (Part-time) [Feb. 2025 - present in Tokyo, Japan]**<br>
Japanese sake sales and customer service

* **Machine Learning Research Engineer in [LINE Corp.](https://linecorp.com/en/) [Oct. 2022 - Sep. 2023 in Tokyo, Japan]**<br>
  Computer Vision

* **Technical Advisor in [BizTech, Inc.](https://www.biz-t.jp/) [Apr. 2022 - present in Japan]**<br>

* **Machine Learning Engineer in [Softbank Corp.](https://www.softbank.jp/en/) [Apr. 2021 - Sep. 2022 in Tokyo, Japan]**<br>
  Computer Vision / Edge Device Development / Backend Engineer

* **Backend Engineer developing [AIC website](https://aic.keio.ac.jp/forStudents/web) [Sep. 2020 - present in Japan]**<br>

* **Internship in [Tenchijin Inc.](https://tenchijin.co.jp/) [Jun. 2020 ‚Äì Mar. 2021  in Tokyo, Japan]**<br>
  Backend / Machine Learning Engineer working on the analysis of the big data about space.

* **Internship in [CyberAgent, Inc.](https://www.cyberagent.co.jp/en/) [Feb. 2020 - Feb. 2020  Tokyo, Japan]**<br>
  Worked on developing AdTech using machine learning and GCP.

* **Internship in [SoftBank Corp.](https://www.softbank.jp/en/) [Aug. 2019 ‚Äì Sep. 2019 in Tokyo, Japan]**<br>
  Worked on the following two task:<br>
  1. classifying a product into normal one or abnormal one and visualizing where a CNN model looks<br>
  2. semantic segmentation for super high-resolution images<br>

* **Research Assisstant in [National Institute of Advanced Industrial Science and Technology(AIST)](https://www.aist.go.jp/index_en.html) [Apr. 2019 ‚Äì Mar. 2021 in Tsukuba, Ibaraki, Japan]**<br>
  Research about Machine Learning and Action Recognition under the supervision of [Ph.D. Hirokatsu KATAOKA](http://hirokatsukataoka.net/).

* **Internship in [IBM Japan, Ltd.](https://www.ibm.com/ibm/jp/en/) [Sep. 2018 - Mar. 2019 in Tokyo, Japan]**<br>
  Worked on weakly-supervised affordance detection using the hierarchy between affordances and objects.

</details>

<!-- Publications -->
<details>
<summary>Publications</summary>

#### Preprint

* **Yuchi Ishikawa**, Toranosuke Manabe, Tatsuya Komatsu, Yoshimitsu Aoki, "Listening without Looking: Modality Bias in Audio-Visual Captioning", in arXiv 2025.

* Toranosuke Manabe, **Yuchi Ishikawa**, Hokuto Munakata, Tatsuya Komatsu, "ProLAP: Probabilistic Language-Audio Pre-Training", in arXiv 2025. [paper](https://arxiv.org/abs/2510.18423)

#### International Conference

* Tatsuya Komatsu, Hokuto Munakata, **Yuchi Ishikawa**, "Leveraging Unlabeled Audio for Audio-Text Contrastive Learning via Audio-Composed Text Features", in Interspeech 2025.

* **Yuchi Ishikawa**, Shota Nakada, Hokuto Munakata, Kazuhiro Saito, Tatsuya Komatsu, Yoshimitsu Aoki, "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", in Interspeech 2025. [paper](https://arxiv.org/abs/2507.11967)

* **Yuchi Ishikawa**, Tatsuya Komatsu, Yoshimitsu Aoki, "Pre-training with Synthetic Patterns for Audio", in ICASSP 2025. [paper](https://www.arxiv.org/abs/2410.00511)

* **Yuchi Ishikawa**, Masayoshi Kondo, Yoshimitsu Aoki, "Data Collection-free Masked Video Modeling" in ECCV 2024. [paper](https://arxiv.org/abs/2409.06665)

* **Yuchi Ishikawa**, Masayoshi Kondo, Hirokatsu Kataoka, "Learnable Cube-based Video Encryption for Privacy-Preserving Action Recognition" in WACV 2024. [paper](https://openaccess.thecvf.com/content/WACV2024/papers/Ishikawa_Learnable_Cube-Based_Video_Encryption_for_Privacy-Preserving_Action_Recognition_WACV_2024_paper.pdf)

* Shuhei Yokoo, Peifei Zhu, **Yuchi Ishikawa**, Mikihiro Tanaka, Masayoshi Kondo, Hirokatsu Kataoka, "Leveraging Image-Text Similarity and Caption Modification for the DataComp Challenge: Filtering Track and BYOD Track" in ICCV 2023 Workshop on Towards the Next Generation of Computer Vision Datasets: DataComp Track. [arXiv](https://arxiv.org/abs/2310.14581)

* Kensho Hara, **Yuchi Ishikawa**, Hirokatsu Kataoka, "Rethinking Training Data for Mitigating Representation Biases in Action Recognition" in CVPR 2021 Workshop on Large Scale Holistic Video Understanding 2021

* **Yuchi Ishikawa**, Seito Kasai, Yoshimitsu Aoki, Hirokatsu kataoka, "Alleviating Over-segmentation Errors by Detecting Action Boundaries" in WACV 2021. [arXiv](https://arxiv.org/abs/2007.06866)

* Seito Kasai, **Yuchi Ishikawa**, Masaki Hayashi, Yoshimitsu Aoki, Kensho Hara, Hirokatsu Kataoka, ‚ÄúRETRIEVING AND HIGHLIGHTING ACTION WITH SPATIOTEMPORAL REFERENCE‚Äù in IEEE ICIP 2020. [arXiv](https://arxiv.org/abs/2005.09183?context=cs)

* **Yuchi Ishikawa**, Haruya Ishikawa, Shuichi Akizuki, Masaki Yamazaki, Yasuhiro Taniguchi, Yoshimitsu Aoki, "Task-oriented Function Detection Based on Operational Tasks" in Conference: 2019 19th International Conference on Advanced Robotics (ICAR). (Acceptance Rate 55.3%)

* Seito Kasai\*, **Yuchi Ishikawa\***, Tenga Wakamiya, Kensho Hara, Hirokatsu Kataoka, ‚ÄúAIST Team submission for Task 3, Dense-Captioning Events in Videos,‚Äù  in CVPR 2019 Workshop, International Challenge on ActivityNet Challenge, 2019.

* Tenga Wakamiya, Kensho Hara, **Yuchi Ishikawa**, Seito Kasai, Hirokatsu Kataoka, ‚ÄúAIST Submission for ActivityNet Challenge 2019 in Trimmed Activity Recognition (Kinetics),‚Äù in CVPR 2019 Workshop, International Challenge on ActivityNet Challenge, 2019.

* Haruya Ishikawa, **Yuchi Ishikawa**, Shuichi Akizuki, Yoshimitsu Aoki, "Human-Object Maps for Daily Activity Recognition" in International Conference on Machine Vision Applications(MVA 2019)

#### Journal

* **Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÁßãÊúàÁßÄ‰∏ÄÔºåÈùíÊú®Áæ©Ê∫ÄÔºåÊìç‰Ωú„Çø„Çπ„ÇØÂÖ•Âäõ„Å´Âü∫„Å•„ÅèÁâ©‰Ωì„ÅÆÊ©üËÉΩÈÉ®Êé®ÂÆö, Á≤æÂØÜÂ∑•Â≠¶‰ºö 85Â∑ª12Âè∑ (2019Âπ¥12Êúà5Êó•Áô∫Ë°å)

#### Domestic Conference

* **Áü≥Â∑ù Ë£ïÂú∞**, Â∞èÊùæ ÈÅî‰πü, ‰ª≤Áî∞ ÂãùÂ§™, ÂÆóÂÉè ÂåóÊñó, ÈΩãËó§ ‰∏ªË£ï, ÈùíÊú® Áæ©Ê∫Ä, "Ë¶ñËÅ¥Ë¶öË™çË≠ò„ÅÆ„Åü„ÇÅ„ÅÆ„Éà„É™„É¢„Éº„ÉÄ„É´Â≠¶Áøí„Å®Ëá™Âãï„Éá„Éº„ÇøÁîüÊàê„ÅÆÊèêÊ°à", Á¨¨28ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2025)

* ÈΩãËó§ ‰∏ªË£ï, Â∞èÊùæ ÈÅî‰πü, **Áü≥Â∑ù Ë£ïÂú∞**, ËøëËó§ ÈõÖËä≥,"„Éó„É©„Ç§„Éê„Ç∑„Éº‰øùË≠∑„ÅÆ„Åü„ÇÅ„ÅÆVision Transformer„É¢„Éá„É´„ÅÆÊöóÂè∑ÂåñÊ≥ï", Á¨¨28ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2025)

* **Áü≥Â∑ù Ë£ïÂú∞**, ÈΩãËó§‰∏ªË£ï, ÈùíÊú®Áæ©Ê∫Ä, "ÂãïÁîª„Éá„Éº„Çø„Å®ÁîªÂÉè„Ç≠„É£„Éó„Ç∑„Éß„É≥ÁîüÊàê„ÇíÁî®„ÅÑ„ÅüÈü≥„Å®„ÉÜ„Ç≠„Çπ„Éà„Éö„Ç¢„ÅÆËá™ÂãïÁîüÊàê", Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨31ÂõûÂπ¥Ê¨°Â§ß‰ºö(NLP2025)

* **‚ΩØÂ∑ù Ë£ïÂú∞**Ôºå‚ªò‚Ωä Áæ©Ê∫Ä, ÊöóÂè∑ÂåñÂãïÁîª„Çí‚Ω§„ÅÑ„Åü„Éó„É©„Ç§„Éê„Ç∑„Éº‰øùË≠∑‰∏ã„Åß„ÅÆË¶ñËÅ¥Ë¶ö‚æèÂãïË™çË≠ò, „Éì„Ç∏„Éß„É≥ÊäÄË°ì„ÅÆÂÆüÂà©Áî®„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2024 (ViEW 2024)

* ÈΩãËó§ ‰∏ªË£ïÔºå**‚ΩØÂ∑ù Ë£ïÂú∞**, Pseudo-Motion Video„ÅÆÂ§öÊßòÂåñ„Å´„Çà„ÇãÂãïÁîªË™çË≠ò„É¢„Éá„É´„ÅÆ‰∫ãÂâçÂ≠¶Áøí„ÅÆ‚æºÁ≤æÂ∫¶Âåñ„ÅÆÊ§úË®é, „Éì„Ç∏„Éß„É≥ÊäÄË°ì„ÅÆÂÆüÂà©Áî®„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2024 (ViEW 2024)

* Naoya Nakajima, **Yuchi Ishikawa**, Masayoshi Kondo, ÂãïÁîª„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÅÆ„Åü„ÇÅ„ÅÆDINO„ÅÆÂãïÁîª„Å∏„ÅÆÊã°Âºµ„ÅÆÊ§úË®é, Á¨¨27ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2024)

* **Áü≥Â∑ù Ë£ïÂú∞**, ËøëËó§ ÈõÖËä≥, ÈùíÊú® Áæ©Ê∫Ä, ‰∏âÈáç„Éû„Çπ„ÇØ„ÇíÁî®„ÅÑ„ÅüVideoMAE„ÅÆÂäπÁéáÂåñ, ÂãïÁöÑÁîªÂÉèÂá¶ÁêÜÂÆüÂà©Áî®Âåñ„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2024 (DIA2024)

* **Áü≥Â∑ù Ë£ïÂú∞**, ËøëËó§ ÈõÖËä≥, ÈùíÊú® Áæ©Ê∫Ä, Ëá™ÁÑ∂ÂãïÁîª„ÇíÁî®„ÅÑ„Å™„ÅÑÂãïÁîªË™çË≠ò„É¢„Éá„É´„ÅÆËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí, ÂãïÁöÑÁîªÂÉèÂá¶ÁêÜÂÆüÂà©Áî®Âåñ„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2024 (DIA2024)

* **Áü≥Â∑ù Ë£ïÂú∞**, ËøëËó§ ÈõÖËä≥, ÈùíÊú® Áæ©Ê∫Ä, Training Video Masked Autoencoder from Static Images, in „Éì„Ç∏„Éß„É≥ÊäÄË°ì„ÅÆÂÆüÂà©Áî®„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2023 (ViEW 2023)

* **Yuchi Ishikawa**, Masayoshi Kondo, Hirokatsu Kataoka, Video and Model Encryption for Privacy-Preserving Action Recognition, Á¨¨26ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2023)

* **Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÁßãÊúàÁßÄ‰∏ÄÔºåÈùíÊú®Áæ©Ê∫ÄÔºåAction Segmentation „Å´„Åä„Åë„ÇãÊêçÂ§±Èñ¢Êï∞„ÅÆÊ§úË®ºÔºåÁ¨¨26ÂõûÁîªÂÉè„Çª„É≥„Ç∑„É≥„Ç∞„Ç∑„É≥„Éù„Ç∏„Ç¶„É†(SSII 2020)

* Seito KASAI, **Yuchi ISHIKAWA**, Tenga WAKAMIYA, Kensho HARA, Hirokatsu KATAOKA, Exploring the Best Model for Dense Captioning Events in Videos, Á¨¨22ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2019)

* Ëã•ÂÆÆÂ§©ÈõÖÔºåÂéüÂÅ•ÁøîÔºå**Áü≥Â∑ùË£ïÂú∞**ÔºåÁ¨†‰∫ïË™†ÊñóÔºå‰∏≠Êùë ÊòéÁîüÔºåÁâáÂ≤° Ë£ïÈõÑÔºåË∂ÖÂ§ßË¶èÊ®°„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Çà„ÇãÂãïÁîªÂÉèË™çË≠ò„ÅÆ„Åü„ÇÅ„ÅÆÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆÊßãÁØâÔºåÁ¨¨22ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2019)

* **Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÁßãÊúàÁßÄ‰∏ÄÔºåÈùíÊú®Áæ©Ê∫ÄÔºå„É≠„Éõ„Çô„ÉÉ„Éà„ÅÆÁâ©‰ΩìÊìç‰Ωú„ÅÆ„Åü„ÇÅ„ÅÆ„Çø„Çπ„ÇØÊåáÂêë„Å™Ê©üËÉΩÈÉ®„ÅÆÊé®ÂÆöÔºåÁ¨¨22ÂõûÁîªÂÉè„ÅÆË™çË≠ò„ÉªÁêÜËß£„Ç∑„É≥„Éù„Ç∏„Ç¶„É† (MIRU2019)

* ÁßãÊúàÁßÄ‰∏ÄÔºå**Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÈùíÊú®Áæ©Ê∫ÄÔºåÁâ©‰Ωì„ÅÆÈÖçÁΩÆË®òËø∞„Å´Âü∫„Å§„Çô„Åè„Ç∑„Éº„É≥Âæ©ÂÖÉ„ÅÆ„Åü„ÇÅ„ÅÆÊìç‰ΩúÊñπÊ≥ïÊé®ÂÆöÔºåÁ¨¨25ÂõûÁîªÂÉè„Çª„É≥„Ç∑„É≥„Ç∞„Ç∑„É≥„Éù„Ç∏„Ç¶„É†(SSII 2019)

* **Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÁßãÊúàÁßÄ‰∏ÄÔºåÈùíÊú®Áæ©Ê∫ÄÔºåÊìç‰Ωú„Çø„Çπ„ÇØ„Å®ÊåáÁ§∫ÂØæË±°„ÇØ„É©„Çπ„ÅÆÂÖ•Âäõ„Å´„Çà„ÇãÁâ©‰Ωì„ÅÆÊ©üËÉΩÈÉ®Êé®ÂÆöÔºåÁ¨¨25ÂõûÁîªÂÉè„Çª„É≥„Ç∑„É≥„Ç∞„Ç∑„É≥„Éù„Ç∏„Ç¶„É†(SSII 2019)

* **Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÁßãÊúàÁßÄ‰∏ÄÔºåÈùíÊú®Áæ©Ê∫ÄÔºåÊìç‰Ωú„Çø„Çπ„ÇØÂÖ•Âäõ„Å´Âü∫„Å•„ÅèÁâ©‰Ωì„ÅÆÊ©üËÉΩÈÉ®Êé®ÂÆöÔºåÂãïÁöÑÁîªÂÉèÂá¶ÁêÜÂÆüÂà©Áî®Âåñ„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2019(DIA 2019)

* ÁßãÊúàÁßÄ‰∏ÄÔºå**Áü≥Â∑ùË£ïÂú∞**ÔºåÁü≥Â∑ùÊô¥‰πüÔºåÈùíÊú®Áæ©Ê∫ÄÔºåÈùûÂÆöÂ∏∏Áä∂ÊÖã„ÅÆÁêÜËß£„Å®„Ç∑„Éº„É≥Âæ©ÂÖÉ„ÅÆ„Åü„ÇÅ„ÅÆÁâ©‰Ωì„ÅÆÊìç‰ΩúÊñπÊ≥ïÊé®ÂÆöÔºåÂãïÁöÑÁîªÂÉèÂá¶ÁêÜÂÆüÂà©Áî®Âåñ„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó2019(DIA 2019)
</details>

<!-- Projects -->
<details>
<summary>Projects</summary>

* **[cvpaper.challenge Á†îÁ©∂ÂäπÁéáÂåñ Tips](https://www.slideshare.net/cvpaperchallenge/cvpaperchallenge-tips-241914101)**<br>
  I wrote about how to efficiently manage experiment results.
  
* **[ÂãïÁîªË™çË≠ò „É°„Çø„Çµ„Éº„Éô„Ç§](https://www.slideshare.net/cvpaperchallenge/v1-232973484)** [May. 2020]<br>
  I read papers on Video Recognition and summarized them.

* **[RADTorch](https://github.com/radtorch/radtorch) Contributor**<br>
  My codes for explainable AI are used in RADTorch.

* **[CVPR2019ÈÄüÂ†±](https://www.slideshare.net/cvpaperchallenge/cvpr-2019)**<br>
  I took part in CVPR 2019 and wrote this article with members of cvpaper.challenge.

* **[cvpaper.challenge](http://xpaperchallenge.org/cv/) [Apr. 2019 ‚Äì present]**<br>
  As a member of cvpaper.challenge, I read a lot of papers accepted at CVPR or several top conferences. I also reseach and share the knowledge with its members.

* **[ActivityNet Challenge](http://activity-net.org/) [Jun. 2019]**<br>
  Out team took part in ActivityNet Challenge in CVPR workshop. We won 9th place in Task A - Trimmed Action Recognition. Our team also participated in Task 3 - Dense-Captioning Events in Videos.

* **[Paper Summary](https://github.com/yiskw713/paper_summary)**<br>
  I read papers every day and summarize them as far as possible in GitHub. If you get interested, visit [my project page](https://github.com/yiskw713/paper_summary).

</details>

<!-- Article -->
<details>
<summary>Articles</summary>

* [Seven papers accepted at ICASSP 2025](https://research.lycorp.co.jp/en/news/283)

* [3 papers accepted at ECCV 2024](https://research.lycorp.co.jp/en/news/250)

* **Interview with a manager of the AI department in Softbank**<br>
  [SFÊò†Áîª„ÅÆ‰∏ñÁïå„ÅØ„ÇÇ„ÅÜÂÆüÁèæ„Åó„Å¶„ÅÑ„ÇãÔºü „ÇΩ„Éï„Éà„Éê„É≥„ÇØ AIÈÉ®ÈñÄ ÈÉ®Èï∑„Å®AIÂ∞ÇÊîªÂ≠¶Áîü„ÅÆÂØæË´á](https://www.softbank.jp/sbnews/entry/20191108_01)

</details>

<!-- Recognition -->
<details>
<summary>Recognition</summary>

* **[Êó•Êú¨ÈÖíÊ§úÂÆö 2Á¥ö](https://ssi-w.com/nihonsyu-kentei/) [Feb. 2025]**

* **[Êó•Êú¨ÈÖíÊ§úÂÆö 3Á¥ö](https://ssi-w.com/nihonsyu-kentei/) [Jan. 2025]**

* **[„Ç≥„Éº„Éí„Éº„Ç§„É≥„Çπ„Éà„É©„ÇØ„Çø„ÉºÊ§úÂÆö 2Á¥ö](https://kentei.jcqa.org/index.asp) [Jun. 2024]**
  
* **[PythonZen & PEP 8 Ê§úÂÆöË©¶È®ì](https://pythonzen-pep8-exam.jp/) [Mar. 2022]**
  
* **[JDLA Deep Learning for ENGINEER](https://www.jdla.org/en/) [Feb. 2022]**

* **[Python 3 „Ç®„É≥„Ç∏„Éã„Ç¢Ë™çÂÆö„Éá„Éº„ÇøÂàÜÊûêË©¶È®ì](https://www.pythonic-exam.com/exam/analyist) [Aug. 2021]**

* **[AWS Cloud Practitioner](https://aws.amazon.com/jp/certification/certified-cloud-practitioner/) [Jul. 2021]**

* **[Python 3 „Ç®„É≥„Ç∏„Éã„Ç¢Ë™çÂÆöÂü∫Á§éË©¶È®ì](https://www.pythonic-exam.com/exam/basic) [Jul. 2021]**

* **[Practical Algorithm Skill Test](https://past.atcoder.jp/) [May. 2020]**<br>
  Rank: ADVANCED (the second highest rank)

* **[Information Technology Passport Examination (IT Passport)](https://www3.jitec.ipa.go.jp/JitesCbt/index.html) [Jan. 2020]**<br>
  National Examination on Information Technology in Japan

* **[JDLA Deep Learning for GENERAL](https://www.jdla.org/en/) [Jul. 2019]**<br>
  JDLA aims to develop Deep Learning Generalist, capable of utilizing in business, which has sufficient knowledge in Deep Leaning.

* **Outstanding Performance Award for Bachelor Theses [Mar. 2019]**<br>
  My bachelor thesis topic is ‚ÄúTask-oriented Function Detection Based on Operational Tasks‚Äù.
  I proposed an alternative representation to Affordance, ‚ÄúTask-oriented Function‚Äù, in the paper. This representation makes it possible to desribe a variety of ways to use an object, though only one usage can be described in Affordance Detection in the field of Computer Vision.

* **3rd place award in MIRU 2018 Young Researchers Program [Aug. 2018]**<br>
  As a young researcher program in MIRU 2018, participants were divided into several groups and each group read papers in the field outside Computer Vision. Then, each group summarized the history, the trend and the conection with Computer Vision. It helped us not only understand different fields, but also consider how we can make use of knowledge about them for Computer Vision.<br>
  Our group, C, read papers in the field of Robotics, focusing on ‚ÄúTransferring Knowledge from Simulation to Real‚Äù. You can see our poster and presentation material from [this link](https://sites.google.com/view/miru2018sapporo/wakate_top/%E5%90%84%E3%83%81%E3%83%BC%E3%83%A0%E3%81%AE%E7%99%BA%E8%A1%A8%E8%B3%87%E6%96%99?authuser=0).

* **[TOEIC](https://www.iibc-global.org/english.html) [May 2018]**<br>
  Score: 940

</details>

<!-- Grants -->
<details>
<summary>Grants</summary>

* **[ÊÖ∂ÂøúÂ∑•Â≠¶‰ºöËÇ≤Ëã±Â•®Â≠¶Áîü](http://www.keiokougakukai.org/) [Apr. 2021 - Mar. 2022]**

* **[Scholarship of Japan Student Services Organization](https://www.jasso.go.jp/shogakukin/index.html) [Apr. 2019 - Mar. 2021]**<br>
  This educational lender have been totally exempted because my achievement during master program was highly evaluated.

* **JEES„Éª„ÇΩ„Éï„Éà„Éê„É≥„ÇØAI‰∫∫ÊùêËÇ≤ÊàêÂ•®Â≠¶Èáë [Apr. 2019 - Mar. 2020]**<br>
  I received this scholorship which aims at supporting up to a hundred of capable students studying Artificial Intelligence in Japan.

</details>

### GitHub Streaks üèÉ‚Äç‚ôÇÔ∏è

[![GitHub Streak](http://github-readme-streak-stats.herokuapp.com?user=yiskw713&theme=gotham&hide_border=true)](https://git.io/streak-stats)

### GitHub Stats ‚ú®

<img align="left" src="https://github-readme-stats.vercel.app/api?username=yiskw713&count_private=true&show_icons=true&theme=gotham&include_all_commits=true" />
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=yiskw713&hide=jupyter%20notebook&layout=compact&theme=gotham" />

<br />

### GitHub Trophy üèÜ

[![trophy](https://github-profile-trophy.vercel.app/?username=yiskw713&theme=alduin&column=8)](https://github.com/ryo-ma/github-profile-trophy)

### Latest Blog Posts üìï

<!-- BLOG-POST-LIST:START -->
- [„ÄêPython„ÄëÁâπÂÆö„ÅÆË°å„Å†„ÅëBlack„Å´„Çà„Çã„Ç≥„Éº„ÉâÊï¥ÂΩ¢„ÇíË°å„Çè„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„Çã](https://yiskw713.hatenablog.com/entry/2023/05/08/190000)
- [„ÄêPython„ÄëÂÆöÊï∞ÁÆ°ÁêÜ„ÅÆ„Åü„ÇÅ„ÅÆ„ÄÅÂÄ§„ÅÆËøΩÂä†„ÉªÂ§âÊõ¥„Çí‰∏çÂèØ„Å´„Åô„Çã„ÇØ„É©„Çπ„ÇíÂÆüË£Ö„Åô„Çã](https://yiskw713.hatenablog.com/entry/2023/05/05/182048)
- [„Äênumpy„ÄëÁâπÂÆö„ÅÆÁØÑÂõ≤„Å†„Åë„Å´seed„ÇíË®≠ÂÆö„Åô„Çã](https://yiskw713.hatenablog.com/entry/2023/02/06/190000)
- [„ÄêLinux„Äë„Éè„Ç§„Éï„É≥„Åã„ÇâÂßã„Åæ„Çã„Éï„Ç°„Ç§„É´„ÇíÊåáÂÆö„Åô„Çã](https://yiskw713.hatenablog.com/entry/2023/02/03/120000)
- [„ÄêPython„Äëbox„Å´ÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çí‰∏ÄÊã¨„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åô„Çã](https://yiskw713.hatenablog.com/entry/2023/01/30/200000)
<!-- BLOG-POST-LIST:END -->

### GitHub Activities

<!--START_SECTION:activity-->
1. üó£ Commented on [#1](https://github.com/yiskw713/paper-summary-tweet-notifier/issues/1) in [yiskw713/paper-summary-tweet-notifier](https://github.com/yiskw713/paper-summary-tweet-notifier)
2. üó£ Commented on [#2](https://github.com/yiskw713/paper-summary-tweet-notifier/issues/2) in [yiskw713/paper-summary-tweet-notifier](https://github.com/yiskw713/paper-summary-tweet-notifier)
3. üí™ Opened PR [#21](https://github.com/yiskw713/pytorch_template/pull/21) in [yiskw713/pytorch_template](https://github.com/yiskw713/pytorch_template)
4. üí™ Opened PR [#8](https://github.com/yiskw713/cv_utils/pull/8) in [yiskw713/cv_utils](https://github.com/yiskw713/cv_utils)
5. ‚ùóÔ∏è Opened issue [#171](https://github.com/yiskw713/paper_summary/issues/171) in [yiskw713/paper_summary](https://github.com/yiskw713/paper_summary)
<!--END_SECTION:activity-->

[googlescholar]: https://scholar.google.com/citations?user=IEF2iOkAAAAJ&hl=en&oi=ao
[twitter]: https://twitter.com/yiskw713
[instagram]: https://www.instagram.com/yciskw_/
[linkedin]: https://www.linkedin.com/in/yiskw713/
[facebook]: https://www.facebook.com/yuchi.ishikawa.7
[blog]: https://yiskw713.hatenablog.com/
